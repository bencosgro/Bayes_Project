{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center> Cancerous Tumor Prediction</center></h1>\n",
    "\n",
    "DS 6014\n",
    "\n",
    "Dr. Donald E. Brown\n",
    "\n",
    "December 8, 2020\n",
    "\n",
    "Project Report\n",
    "\n",
    "Benjamin Cosgro, Pantea Ferdosian, Joon Yuhl Soh\n",
    "\n",
    "### Problem description:\n",
    "\n",
    "The problem that our group decided to work on is predicting the malignancy of cancer tumors based on the features that they have. Our purpose is to find out how much these features can help us in order to predict such important information for any potential patients. \n",
    "\n",
    "\n",
    "\n",
    "### Mathematical Linkange between the problem and the method\n",
    "\n",
    "\n",
    "For this project, we chose to model the training set using the GLM logistic regression in pymc3 using the mean, standard error, and perform the testing on the \"worst\" features. \n",
    "\n",
    "\n",
    "We cleaned the dataset, analyzed the possible correlations, and used the variance inflation factor (VIF) method in order to diagnose multicollinearity.    \n",
    "\n",
    "After doing the above steps, we assumed that the probability of tumor being *malignant* is a function of *concavity* and *symmetry* when using the mean, standatrd error.\n",
    "\n",
    "We get the following interpretation formula: \n",
    "\n",
    "$$logit = z = \\beta _0 + \\beta_1(concavity) +\\beta_2 (symmetry)$$\n",
    "\n",
    "$ y = 1$ or $y=0$ whethere the tumor is benign or malignant.\n",
    "\n",
    "And the likelihood or $p(Concavity|Malignance)$ is given by $\\prod_{1=1}^{n} p_i^y(1-p_i)^{1-y_i}$ where $p$ is the probability of output\n",
    "\n",
    "$$p = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "For this problem, we are are attempting to find the probability that a tumor is benign or malignant given the features mentioned in the dataset $$p(Malignance|Concavity)$$\n",
    "\n",
    "The `.glm()` function in pymc3 would do all the modeling for us using the Hamiltonian Monte Carlo method called the NUTS (no u-turn sampling) and in this case, the parameters would be tuned automatically. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
